description: robust cifar10 run on philly.

target:
  vc: msrlabs
  cluster: wu2

environment:
  image: pengchuanzhang/maskrcnn:py3.7-cuda10.0-pytorch1.7

code:
  local_dir: $CONFIG_DIR/../

storage:
  my_storage:
    # Replace with the name of the Azure Storage Account you created.
    #   If you followed the tutorial, this should be your_username
    # storage_account_name: penzhaneu
    storage_account_name: penzhanwu2
    # storage_account_name: penzhanscus
    # Specify a name of the container on your blob (ex. phillytools),
    #   to store data, results and code. It will be created if it does not exist.
    container_name: results
    # container_name: experiments
    # You can optionally specify a mount_path that will be directly accessible
    #   by your jobs. By default it's:
    #mount_path: /mnt/_default
    is_output: True

data:
  #data_upload: True
  local_dir: $CONFIG_DIR/datasets
  remote_dir: data


search:
  type: grid
  max_trials: 200
  params:
  - name: ARCH
    spec: discrete
    values: [resnet18, resnet50, deit_tiny_patch4_32]
  - name: LR
    spec: discrete
    values: [1e-1]
  - name: WD
    spec: discrete
    values: [5e-4]
  - name: BSZ
    spec: discrete
    values:
    - 256
  - name: EPOCH
    spec: discrete
    values:
    - 150
  - name: AT
    spec: discrete
    values: [0, 1]
  job_template:
    name: arch{ARCH}_AT{AT}
    sku: G1
    sku_count: 1
    command:
    - export MKL_THREADING_LAYER='GNU'
    - python main_trainer.py 
          --arch {ARCH} 
          --dataset cifar 
          --batch-size {BSZ} 
          --weight-decay {WD}     
          --out-dir AT_cifar
          --resume 
          --exp-name wide_resnet50_2_eps_0.5_step_0.4_steps_3 
          --constraint inf
          --attack-steps 7 
          --adv-train {AT}
          --eps 0.03 
          --attack-lr 0.5
          --log-iter 1
          --epochs {EPOCH}
          --step-lr 50
          --workers 8
          --lr 0.1
    submit_args:
      max_attempts: 5
      container_args:
        shm_size: 64G