description: robust cifar10 run on philly.

target:
  _name: itpeusp100cl
  service: amlk8s
  vc: resrchvc
  cluster: itpeastusv100cl
  subscription_id: 46da6261-2167-4e71-8b0d-f4a45215ce61
  workspace_name: resrchvc-eus
  resource_group: researchvc-eus
  metadata:
    gpu_types:
    - P100
    num_gpus_per_vm: 4

environment:
  image: pengchuanzhang/maskrcnn:py3.7-cuda10.0-pytorch1.7

code:
  local_dir: $CONFIG_DIR/../

storage:
  my_storage:
    # Replace with the name of the Azure Storage Account you created.
    #   If you followed the tutorial, this should be your_username
    storage_account_name: penzhaneu
    # storage_account_name: penzhanwu2
    # storage_account_name: penzhanscus
    # Specify a name of the container on your blob (ex. phillytools),
    #   to store data, results and code. It will be created if it does not exist.
    container_name: results
    # container_name: experiments
    # You can optionally specify a mount_path that will be directly accessible
    #   by your jobs. By default it's:
    #mount_path: /mnt/_default
    is_output: True

data:
  #data_upload: True
  local_dir: $CONFIG_DIR/datasets
  remote_dir: data


search:
  type: grid
  max_trials: 200
  params:
  - name: ARCH
    spec: discrete
    values: [resnet18, resnet50, deit_tiny_patch4_32, deit_small_patch4_32, deit_base_patch4_32]
  - name: WD
    spec: discrete
    values: [1e-6, 1e-5, 1e-4, 2e-4, 5e-4]
  - name: BSZ
    spec: discrete
    values:
    - 256
  - name: EPOCH
    spec: discrete
    values:
    - 150
  - name: AT
    spec: discrete
    values: [0, 1]
  - name: LR
    spec: discrete
    values: [.1, .5, 1]
  job_template:
    name: arch{ARCH}_AT{AT}_WD{WD}_LR{LR}
    sku: G1
    sku_count: 1
    command:
    - export MKL_THREADING_LAYER='GNU'
    - pip install dill GPUtil tables seaborn cox
    - python main_trainer.py 
          --arch {ARCH} 
          --dataset cifar 
          --batch-size {BSZ} 
          --weight-decay {WD}     
          --resume 
          --constraint inf
          --attack-steps 7 
          --adv-train {AT}
          --eps 0.03 
          --attack-lr 0.005
          --log-iter 1
          --epochs {EPOCH}
          --step-lr 50
          --workers 8
          --lr {LR}
          --exp-name results
    submit_args:
      max_attempts: 5
      container_args:
        shm_size: 64G